Clustering with sandboxes on windows

go to mysqlshell (mysqlsh)
Deploy New Sandbox Instances
code:
dba.deploySandboxInstance("3310")
dba.deploySandboxInstance("3320")
dba.deploySandboxInstance("3330")
dba.deploySandboxInstance("3340")
âœ” All instances deployed successfully
âœ” All used same root password
âœ” All running on localhost inside your user directory



\connect to main or primary node want to make
\connect root@localhost:3310
var x = dba.createCluster("mycluster")
âœ” This created the primary cluster node.
âœ” Status showed cluster NOT tolerant yet.



Add Remaining Nodes to the Cluster
Added remaining sandbox instances:

x.addInstance("root@localhost:3320")
x.addInstance("root@localhost:3330")
x.addInstance("root@localhost:3340")

if error comes run 
x.checkInstanceConfiguration("")
x.configureInstance("")

Verified Cluster Health
x.status()

open new command prompt 
Bootstrap MySQL Router
Executed on Windows:
mysqlrouter --bootstrap root@localhost:3310 -d %HOMEPATH%\mysql-router
âœ” Router config generated
âœ” Ports assigned:
Classic protocol
R/W â†’ 6446
R/O â†’ 6447 
X protocol
R/W â†’ 6448
R/O â†’ 6449

 Started Router
start "" /B "C:\Program Files\MySQL\MySQL Router 8.0\bin\mysqlrouter.exe" ^
    -c "%HOMEPATH%\mysql-router\mysqlrouter.conf"
âœ” Router started in background


Connected Through Router
Connected to R/W router port:
mysqlsh root@localhost:6446
Verified:
x = dba.getCluster()
x.status()


Tested HA + Automatic Failover
âœ” Killed primary node:
dba.killSandboxInstance(3310)
âœ” Router automatically routed next connection to 3320
SELECT @@port;
-- returned 3320



âœ” Cluster showed node 3310 as MISSING
âœ” Restarted node:
dba.startSandboxInstance(3310)
âœ” Node rejoined as SECONDARY
âœ” Cluster became healthy again



Tested Database Replication Through Router
Connected:
mysqlsh root@localhost:6446
Created DB + table:

create database school;
use school;
create table students(id int primary key, name varchar(50));
insert into students values
  (1,'Aakash'), (2,'Krishna'), (3,'RouterTest');


âœ” Replicated instantly to all nodes
âœ” Verified via read-only port (6447)


After single clustering we can create multiple 
clusters with multiple cluster and nodes so if a
complete cluster gets corrupted then we can switch it to another
cluster for the time being.
Multi-Clustering (DR Site), ClusterSet & Full Failover Notes
 
 
Create DR Sandbox Cluster (Replica Site)
Deploy second-site sandbox instances:

dba.deploySandboxInstance("3410")
dba.deploySandboxInstance("3420")
dba.deploySandboxInstance("3430")
âœ” These will become mycluster2 (DR cluster).
âœ” Use same root password.



Convert Single Cluster into a ClusterSet (Global HA/DR)
Connected to PRIMARY site:
var x = dba.getCluster()
var cs = x.createClusterSet("globalcs")

âœ” Now ClusterSet exists with 1 primary cluster (mycluster)
âœ” We still need to add the DR cluster.



Create DR Cluster (mycluster2) inside ClusterSet
Run:
cs.createReplicaCluster("root@localhost:3410", "mycluster2")

âœ” Automatically cloned data from PRIMARY
âœ” Created mycluster2 with PRIMARY at 3410



Then add other DR nodes:
var r = dba.getCluster("mycluster2")
r.addInstance("root@localhost:3420")
r.addInstance("root@localhost:3430")

Check DR cluster:
r.status()


Verify Complete ClusterSet Health
var cs = dba.getClusterSet()
cs.status()
Expected:
mycluster = PRIMARY
mycluster2 = REPLICA
Status = HEALTHY

now for testing API with which we will create a server.js file using node.js
and we will run it and test its api 
http://localhost:3000/students 
for get post put delete
while checking it for multiple failovers




Test 1 â€” Kill PRIMARY node of Site-A (3310)
dba.killSandboxInstance(3310)
Check cluster:
x.status()
âœ” 3320 became PRIMARY
âœ” Router continued routing
âœ” API still worked


Test 2 â€” Kill Entire Primary Cluster (3310â€“3340)
Simulating Full Data Center Outage ðŸ”¥ðŸ”¥ðŸ”¥
dba.killSandboxInstance(3310)
dba.killSandboxInstance(3320)
dba.killSandboxInstance(3330)
dba.killSandboxInstance(3340)
Check status:
cs.status()
Shows PRIMARY unreachable.



Promote DR Cluster to Global PRIMARY
cs.forcePrimaryCluster("mycluster2")
Expected:
mycluster2 becomes PRIMARY
mycluster marked as INVALIDATED
Router starts routing to 3410
âœ” API still works
âœ” Inserts/updates succeed
âœ” Zero outage except failover time (< 1 sec)

ðŸ” Recovery of Old Primary Cluster
Bring Site-A Back Online

Start all old nodes:

dba.startSandboxInstance(3310)
dba.startSandboxInstance(3320)
dba.startSandboxInstance(3330)
dba.startSandboxInstance(3340)

Reboot Invalidated Cluster

Connect to one node:

\connect root@localhost:3310
dba.rebootClusterFromCompleteOutage()


âœ” mycluster restored
âœ” Only 3330 becomes PRIMARY (highest GTID)

Rejoin Recovered Cluster back to ClusterSet

From global current PRIMARY (3410):

\connect root@localhost:3410
var cs = dba.getClusterSet()
cs.rejoinCluster("mycluster")

ðŸ”„ Switching Primary Back to Site-A

Once rejoined:

cs.setPrimaryCluster("mycluster")


âœ” mycluster promoted
âœ” mycluster2 becomes REPLICA
âœ” All replication channels updated
âœ” Router automatically switches back

ðŸ§ª Final Verification
Primary cluster status
x = dba.getCluster("mycluster")
x.status()

ClusterSet status
cs.status()

API test

Perform:

POST /students

GET /students

âœ” Should work
âœ” Should be replicated to DR cluster
âœ” Shows full HA + DR resilience

ðŸ§± Summary of What You Accomplished

You built a complete enterprise-grade HA + DR MySQL architecture:

âœ” 4-node Primary InnoDB Cluster
âœ” 3-node DR Cluster
âœ” ClusterSet controlling global topology
âœ” Automatic failover inside each cluster
âœ” Disaster failover across datacenters
âœ” Transparent routing using MySQL Router
âœ” Application continuity without config changes
âœ” Full recovery + reintegration + re-promotion of old primary

This is EXACTLY how real banks, stock exchanges, hospitals & fintech run MySQL.